# evaluation/evaluate_ragas.py
"""
ƒê√°nh gi√° chatbot TLU v·ªõi RAGAS framework
S·ª≠ d·ª•ng Ollama (local LLM) thay v√¨ OpenAI
"""

import json
import os
import sys
from datetime import datetime

sys.path.append(os.path.dirname(os.path. dirname(os.path.abspath(__file__))))

from datasets import Dataset
from ragas import evaluate
from ragas. metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from ragas. llms import LangchainLLMWrapper
from ragas. embeddings import LangchainEmbeddingsWrapper
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# C·∫•u h√¨nh
CHROMA_PATH = "chroma_db"
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
TEST_DATASET_PATH = "/mnt/48AC6E9BAC6E82F4/Dev/TLUBot/evaluation/test_dataset.json"
RESULTS_PATH = "evaluation/results"
OLLAMA_MODEL = "qwen2.5:7b"

PROMPT_TEMPLATE = """
B·∫°n l√† TLUBot - tr·ª£ l√Ω t∆∞ v·∫•n tuy·ªÉn sinh c·ªßa Tr∆∞·ªùng ƒê·∫°i h·ªçc Th·ªßy l·ª£i. 
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi. 
N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y. 
Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, th√¢n thi·ªán v√† ch√≠nh x√°c. 

Th√¥ng tin tham kh·∫£o:
{context}

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi:
"""


def load_vectorstore():
    """Load ChromaDB"""
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'}
    )
    vectorstore = Chroma(
        persist_directory=CHROMA_PATH,
        embedding_function=embeddings
    )
    return vectorstore


def format_docs(docs):
    """Format documents th√†nh string"""
    return "\n\n".join(doc.page_content for doc in docs)


def create_rag_chain(vectorstore):
    """T·∫°o RAG chain"""
    llm = OllamaLLM(model=OLLAMA_MODEL)
    retriever = vectorstore. as_retriever(
        search_type="similarity",
        search_kwargs={"k": 5}
    )
    prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain, retriever


def load_test_dataset():
    """Load test dataset t·ª´ file JSON"""
    with open(TEST_DATASET_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data['test_cases']


def generate_answers(rag_chain, retriever, test_cases):
    """Generate c√¢u tr·∫£ l·ªùi v√† l·∫•y context cho m·ªói test case"""
    results = []

    print(f"\nüìù ƒêang generate c√¢u tr·∫£ l·ªùi cho {len(test_cases)} test cases...")

    for i, test_case in enumerate(test_cases):
        question = test_case['question']
        ground_truth = test_case['ground_truth']

        print(f"  [{i+1}/{len(test_cases)}] {question[:50]}...")

        # L·∫•y context t·ª´ retriever
        retrieved_docs = retriever.invoke(question)
        contexts = [doc.page_content for doc in retrieved_docs]

        # Generate answer
        answer = rag_chain.invoke(question)

        results.append({
            'question': question,
            'answer': answer,
            'contexts': contexts,
            'ground_truth': ground_truth,
            'category': test_case. get('category', 'unknown')
        })

    return results


def prepare_ragas_dataset(results):
    """Chu·∫©n b·ªã dataset theo format RAGAS"""
    data = {
        'question': [r['question'] for r in results],
        'answer': [r['answer'] for r in results],
        'contexts': [r['contexts'] for r in results],
        'ground_truth': [r['ground_truth'] for r in results]
    }
    return Dataset.from_dict(data)


def setup_ragas_with_ollama():
    """C·∫•u h√¨nh RAGAS s·ª≠ d·ª•ng Ollama thay v√¨ OpenAI"""
    llm = OllamaLLM(model=OLLAMA_MODEL, temperature=0)
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'}
    )

    ragas_llm = LangchainLLMWrapper(llm)
    ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)

    return ragas_llm, ragas_embeddings


def run_ragas_evaluation(dataset):
    """Ch·∫°y ƒë√°nh gi√° RAGAS v·ªõi Ollama"""
    print("\nüîç ƒêang ƒë√°nh gi√° v·ªõi RAGAS (s·ª≠ d·ª•ng Ollama)...")
    print("‚è≥ Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t...")

    ragas_llm, ragas_embeddings = setup_ragas_with_ollama()

    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    ]

    result = evaluate(
        dataset=dataset,
        metrics=metrics,
        llm=ragas_llm,
        embeddings=ragas_embeddings,
    )

    return result


def extract_scores(ragas_result):
    """Tr√≠ch xu·∫•t scores t·ª´ EvaluationResult object"""
    scores = {}
    metric_names = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']

    # S·ª≠ d·ª•ng to_pandas() v√¨ ƒë√£ x√°c nh·∫≠n ho·∫°t ƒë·ªông
    try:
        df = ragas_result.to_pandas()
        print(f"\nüìä DataFrame columns: {df.columns.tolist()}")
        
        for metric in metric_names:
            if metric in df.columns:
                # L·∫•y gi√° tr·ªã mean, b·ªè qua NaN
                values = df[metric]. dropna()
                if len(values) > 0:
                    scores[metric] = float(values.mean())
                else:
                    scores[metric] = 0.0
        
        print(f"‚úÖ Extracted scores: {scores}")
        
    except Exception as e:
        print(f"‚ùå Error extracting scores: {e}")
        # Fallback: tr·∫£ v·ªÅ scores r·ªóng
        scores = {metric: 0.0 for metric in metric_names}

    return scores


def save_results(ragas_scores, generated_answers):
    """L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°"""
    os.makedirs(RESULTS_PATH, exist_ok=True)

    timestamp = datetime.now(). strftime("%Y%m%d_%H%M%S")
    scores_file = os. path.join(RESULTS_PATH, f"ragas_scores_{timestamp}.json")

    # Convert scores to serializable format
    serializable_scores = {}
    for k, v in ragas_scores.items():
        try:
            if v is not None:
                serializable_scores[k] = round(float(v), 4)
            else:
                serializable_scores[k] = None
        except Exception:
            serializable_scores[k] = str(v)

    scores_data = {
        'timestamp': timestamp,
        'overall_scores': serializable_scores,
        'num_test_cases': len(generated_answers),
        'detailed_results': generated_answers
    }

    with open(scores_file, 'w', encoding='utf-8') as f:
        json. dump(scores_data, f, ensure_ascii=False, indent=2, default=str)

    print(f"\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£ t·∫°i: {scores_file}")
    return scores_file


def print_results(ragas_scores):
    """In k·∫øt qu·∫£ ƒë√°nh gi√°"""
    print("\n" + "=" * 60)
    print("üìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å RAGAS (v·ªõi Ollama)")
    print("=" * 60)

    if not ragas_scores:
        print("\n‚ö†Ô∏è Kh√¥ng c√≥ scores ƒë·ªÉ hi·ªÉn th·ªã!")
        print("üí° Th·ª≠ ch·∫°y evaluate_simple.py thay th·∫ø")
        return

    print(f"\n{'Metric':<25} {'Score':<10} {'ƒê√°nh gi√°':<20}")
    print("-" * 55)

    metrics_info = {
        'faithfulness': 'ƒê·ªô trung th·ª±c',
        'answer_relevancy': 'ƒê·ªô li√™n quan',
        'context_precision': 'ƒê·ªô ch√≠nh x√°c context',
        'context_recall': 'ƒê·ªô ƒë·∫ßy ƒë·ªß context',
    }

    total_score = 0.0
    count = 0

    for metric, name in metrics_info. items():
        if metric in ragas_scores:
            score = ragas_scores[metric]
            
            # Chuy·ªÉn ƒë·ªïi score sang float an to√†n
            try:
                if score is None:
                    score_val = 0.0
                else:
                    score_val = float(score)
            except (ValueError, TypeError):
                score_val = 0.0

            # ƒê√°nh gi√° m·ª©c ƒë·ªô
            if score_val >= 0.8:
                rating = "‚úÖ T·ªët"
            elif score_val >= 0.6:
                rating = "‚ö†Ô∏è Kh√°"
            else:
                rating = "‚ùå C·∫ßn c·∫£i thi·ªán"

            # In k·∫øt qu·∫£ - FIX: kh√¥ng c√≥ kho·∫£ng tr·∫Øng trong format specifier
            print(f"{name:<25} {score_val:. 4f}     {rating}")
            
            total_score += score_val
            count += 1

    # T√≠nh ƒëi·ªÉm trung b√¨nh
    if count > 0:
        avg_score = total_score / count
        print("-" * 55)
        print(f"{'ƒêI·ªÇM TRUNG B√åNH':<25} {avg_score:.4f}")

    print("=" * 60)


def main():
    print("üöÄ B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å CHATBOT V·ªöI RAGAS")
    print("üìå S·ª≠ d·ª•ng Ollama (Local LLM) thay v√¨ OpenAI")
    print("=" * 60)

    # 1. Load components
    print("\nüìö ƒêang load vector store...")
    vectorstore = load_vectorstore()
    rag_chain, retriever = create_rag_chain(vectorstore)

    # 2. Load test dataset
    print("üìã ƒêang load test dataset...")
    test_cases = load_test_dataset()
    print(f"   S·ªë l∆∞·ª£ng test cases: {len(test_cases)}")

    # 3.  Generate answers
    generated_answers = generate_answers(rag_chain, retriever, test_cases)

    # 4.  Prepare RAGAS dataset
    print("\nüîß ƒêang chu·∫©n b·ªã dataset cho RAGAS...")
    ragas_dataset = prepare_ragas_dataset(generated_answers)

    # 5.  Run RAGAS evaluation
    ragas_result = run_ragas_evaluation(ragas_dataset)

    # 6. Extract scores t·ª´ result object
    print("\nüìä ƒêang tr√≠ch xu·∫•t k·∫øt qu·∫£...")
    ragas_scores = extract_scores(ragas_result)

    # # 7. Print results
    # print_results(ragas_scores)

    # 8. Save results
    # save_results(ragas_scores, generated_answers)

    print("\nüéâ HO√ÄN TH√ÄNH ƒê√ÅNH GI√Å!")


if __name__ == "__main__":
    main()