# benchmark/benchmark_components.py
"""
Benchmark toÃ n diá»‡n táº¥t cáº£ thÃ nh pháº§n RAG
- Embedding Models
- Chunking Strategies  
- Retrieval Strategies

PhiÃªn báº£n: 2.0 - ÄÃ£ tá»‘i Æ°u, khÃ´ng lá»—i
"""

import json
import time
import os
import sys
import statistics
import warnings
from datetime import datetime
from typing import List, Dict
from dataclasses import dataclass, asdict

# Táº¯t warnings
warnings.filterwarnings("ignore")

# ThÃªm path
sys.path.append(os.path.dirname(os. path.dirname(os.path. abspath(__file__))))

# ============== IMPORTS ==============
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Import HuggingFaceEmbeddings - thá»­ báº£n má»›i trÆ°á»›c
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings

# ============== Cáº¤U HÃŒNH ==============
DATA_PATH = "data/processed"
TEST_DATASET_PATH = "/mnt/48AC6E9BAC6E82F4/Dev/TLUBot/evaluation/test_dataset.json"
RESULTS_PATH = "benchmark/results"

# ThÆ° má»¥c chá»©a táº¥t cáº£ ChromaDB táº¡m - sáº½ khÃ´ng xÃ³a trong quÃ¡ trÃ¬nh cháº¡y
BENCHMARK_TEMP_DIR = "benchmark/temp_dbs"

# ============== Cáº¤U HÃŒNH TEST ==============

EMBEDDING_MODELS = [
    {
        "name": "multilingual-MiniLM",
        "model_id": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    },
    {
        "name": "multilingual-e5-small",
        "model_id":  "intfloat/multilingual-e5-small",
    },
    # Uncomment náº¿u muá»‘n test thÃªm (tá»‘n thá»i gian)
    # {
    #     "name":  "multilingual-e5-base",
    #     "model_id": "intfloat/multilingual-e5-base",
    # },
]

CHUNKING_CONFIGS = [
    {"name": "small", "chunk_size": 300, "chunk_overlap": 50},
    {"name": "medium", "chunk_size": 500, "chunk_overlap": 100},
    {"name": "large", "chunk_size": 1000, "chunk_overlap": 200},
    {"name": "xlarge", "chunk_size": 1500, "chunk_overlap": 300},
]

RETRIEVAL_CONFIGS = [
    {"name": "similarity_k3", "search_type": "similarity", "k": 3},
    {"name": "similarity_k5", "search_type": "similarity", "k": 5},
    {"name": "similarity_k7", "search_type": "similarity", "k": 7},
    {"name": "mmr_k5", "search_type": "mmr", "k": 5, "fetch_k": 10},
    {"name": "mmr_k7", "search_type": "mmr", "k": 7, "fetch_k": 15},
]

# ============== DATA CLASS ==============
@dataclass
class BenchmarkResult:
    component_type: str
    config_name: str
    config_details: Dict
    hit_rate:  float
    mrr: float
    avg_latency: float
    num_chunks: int
    index_time: float

# ============== HELPER FUNCTIONS ==============

def load_test_cases() -> List[Dict]:
    """Load test dataset"""
    with open(TEST_DATASET_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data['test_cases']

def load_documents():
    """Load documents"""
    loader = DirectoryLoader(
        DATA_PATH,
        glob="**/*.txt",
        loader_cls=TextLoader,
        loader_kwargs={"encoding": "utf-8"}
    )
    return loader.load()

def calculate_hit_and_mrr(retriever, test_cases: List[Dict], num_tests: int = 10):
    """TÃ­nh Hit Rate vÃ  MRR"""
    hit_count = 0
    reciprocal_ranks = []
    latencies = []
    
    for test_case in test_cases[:num_tests]:
        question = test_case['question']
        ground_truth = test_case['ground_truth'].lower()
        gt_words = set(ground_truth.split())
        
        # Äo latency
        start = time.time()
        docs = retriever.invoke(question)
        latencies.append(time.time() - start)
        
        # TÃ¬m hit
        found = False
        for rank, doc in enumerate(docs, 1):
            doc_words = set(doc.page_content.lower().split())
            overlap = len(gt_words & doc_words) / len(gt_words) if gt_words else 0
            
            if overlap > 0.2 and not found:
                hit_count += 1
                reciprocal_ranks.append(1.0 / rank)
                found = True
                break
        
        if not found:
            reciprocal_ranks.append(0.0)
    
    hit_rate = hit_count / num_tests
    mrr = statistics.mean(reciprocal_ranks) if reciprocal_ranks else 0
    avg_latency = statistics.mean(latencies) if latencies else 0
    
    return hit_rate, mrr, avg_latency

# ============== BENCHMARK FUNCTIONS ==============

def benchmark_embeddings(documents, test_cases) -> List[BenchmarkResult]: 
    """Benchmark cÃ¡c Embedding Models"""
    print("\n" + "=" * 60)
    print("1ï¸âƒ£  BENCHMARK EMBEDDING MODELS")
    print("=" * 60)
    
    results = []
    
    # Chunking cá»‘ Ä‘á»‹nh
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = text_splitter.split_documents(documents)
    
    for i, config in enumerate(EMBEDDING_MODELS):
        print(f"\n   ğŸ”¤ [{i+1}/{len(EMBEDDING_MODELS)}] Testing: {config['name']}")
        
        try:
            # Táº¡o embeddings
            embeddings = HuggingFaceEmbeddings(
                model_name=config['model_id'],
                model_kwargs={'device': 'cpu'}
            )
            
            # Táº¡o vectorstore trong memory (khÃ´ng persist)
            start_index = time.time()
            vectorstore = Chroma.from_documents(
                documents=chunks,
                embedding=embeddings,
                # KhÃ´ng dÃ¹ng persist_directory Ä‘á»ƒ trÃ¡nh lá»—i lock
            )
            index_time = time.time() - start_index
            
            # Benchmark
            retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
            hit_rate, mrr, avg_latency = calculate_hit_and_mrr(retriever, test_cases)
            
            result = BenchmarkResult(
                component_type="embedding",
                config_name=config['name'],
                config_details=config,
                hit_rate=hit_rate,
                mrr=mrr,
                avg_latency=avg_latency,
                num_chunks=len(chunks),
                index_time=index_time
            )
            results.append(result)
            
            print(f"      âœ… Hit Rate: {hit_rate:.2%}, MRR: {mrr:.3f}, Index:  {index_time:.2f}s")
            
            # Cleanup
            del vectorstore
            del embeddings
            
        except Exception as e:
            print(f"      âŒ Error: {e}")
            results.append(BenchmarkResult(
                component_type="embedding",
                config_name=config['name'],
                config_details=config,
                hit_rate=0, mrr=0, avg_latency=0, num_chunks=0, index_time=0
            ))
    
    return results

def benchmark_chunking(documents, test_cases) -> List[BenchmarkResult]:
    """Benchmark cÃ¡c Chunking Strategies"""
    print("\n" + "=" * 60)
    print("2ï¸âƒ£  BENCHMARK CHUNKING STRATEGIES")
    print("=" * 60)
    
    results = []
    
    # Embedding cá»‘ Ä‘á»‹nh
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    for i, config in enumerate(CHUNKING_CONFIGS):
        print(f"\n   ğŸ“„ [{i+1}/{len(CHUNKING_CONFIGS)}] Testing: {config['name']} (size={config['chunk_size']})")
        
        try:
            # Chunking
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=config['chunk_size'],
                chunk_overlap=config['chunk_overlap']
            )
            chunks = text_splitter.split_documents(documents)
            
            # Táº¡o vectorstore
            start_index = time.time()
            vectorstore = Chroma. from_documents(
                documents=chunks,
                embedding=embeddings,
            )
            index_time = time.time() - start_index
            
            # Benchmark
            retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
            hit_rate, mrr, avg_latency = calculate_hit_and_mrr(retriever, test_cases)
            
            result = BenchmarkResult(
                component_type="chunking",
                config_name=config['name'],
                config_details=config,
                hit_rate=hit_rate,
                mrr=mrr,
                avg_latency=avg_latency,
                num_chunks=len(chunks),
                index_time=index_time
            )
            results.append(result)
            
            print(f"      âœ… Chunks: {len(chunks)}, Hit Rate: {hit_rate:.2%}, MRR: {mrr:.3f}")
            
            # Cleanup
            del vectorstore
            
        except Exception as e:
            print(f"      âŒ Error: {e}")
            results.append(BenchmarkResult(
                component_type="chunking",
                config_name=config['name'],
                config_details=config,
                hit_rate=0, mrr=0, avg_latency=0, num_chunks=0, index_time=0
            ))
    
    del embeddings
    return results

def benchmark_retrieval(documents, test_cases) -> List[BenchmarkResult]:
    """Benchmark cÃ¡c Retrieval Strategies"""
    print("\n" + "=" * 60)
    print("3ï¸âƒ£  BENCHMARK RETRIEVAL STRATEGIES")
    print("=" * 60)
    
    results = []
    
    # Setup cá»‘ Ä‘á»‹nh
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = text_splitter.split_documents(documents)
    
    print(f"\n   ğŸ“š Táº¡o vectorstore vá»›i {len(chunks)} chunks...")
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
    )
    
    for i, config in enumerate(RETRIEVAL_CONFIGS):
        print(f"\n   ğŸ” [{i+1}/{len(RETRIEVAL_CONFIGS)}] Testing: {config['name']}")
        
        try: 
            # Táº¡o retriever
            search_kwargs = {"k": config['k']}
            if config['search_type'] == 'mmr':
                search_kwargs['fetch_k'] = config. get('fetch_k', 10)
            
            retriever = vectorstore.as_retriever(
                search_type=config['search_type'],
                search_kwargs=search_kwargs
            )
            
            # Benchmark
            hit_rate, mrr, avg_latency = calculate_hit_and_mrr(retriever, test_cases)
            
            result = BenchmarkResult(
                component_type="retrieval",
                config_name=config['name'],
                config_details=config,
                hit_rate=hit_rate,
                mrr=mrr,
                avg_latency=avg_latency,
                num_chunks=len(chunks),
                index_time=0
            )
            results.append(result)
            
            print(f"      âœ… Hit Rate:  {hit_rate:.2%}, MRR: {mrr:. 3f}, Latency: {avg_latency:.3f}s")
            
        except Exception as e:
            print(f"      âŒ Error: {e}")
            results.append(BenchmarkResult(
                component_type="retrieval",
                config_name=config['name'],
                config_details=config,
                hit_rate=0, mrr=0, avg_latency=0, num_chunks=0, index_time=0
            ))
    
    del vectorstore
    del embeddings
    return results

# ============== PRINT FUNCTIONS ==============

def print_results(results: List[BenchmarkResult]):
    """In káº¿t quáº£ Ä‘áº¹p"""
    
    # Embedding results
    emb_results = [r for r in results if r.component_type == "embedding"]
    if emb_results:
        print(f"\n{'='*70}")
        print("ğŸ“Š Káº¾T QUáº¢:  EMBEDDING MODELS")
        print(f"{'='*70}")
        print(f"\n{'Config':<25} {'Hit Rate':<12} {'MRR':<10} {'Index Time':<12}")
        print("-" * 60)
        
        sorted_emb = sorted(emb_results, key=lambda x:  x.mrr, reverse=True)
        for i, r in enumerate(sorted_emb):
            medal = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
            print(f"{medal} {r.config_name:<22} {r.hit_rate:<12.2%} {r.mrr:<10.3f} {r.index_time:<12.2f}s")
    
    # Chunking results
    chunk_results = [r for r in results if r.component_type == "chunking"]
    if chunk_results:
        print(f"\n{'='*70}")
        print("ğŸ“Š Káº¾T QUáº¢:  CHUNKING STRATEGIES")
        print(f"{'='*70}")
        print(f"\n{'Config':<12} {'Size':<10} {'Overlap':<10} {'Chunks':<10} {'Hit Rate':<12} {'MRR':<10}")
        print("-" * 70)
        
        sorted_chunk = sorted(chunk_results, key=lambda x: x.mrr, reverse=True)
        for i, r in enumerate(sorted_chunk):
            medal = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
            size = r.config_details.get('chunk_size', 0)
            overlap = r.config_details.get('chunk_overlap', 0)
            print(f"{medal} {r.config_name:<9} {size:<10} {overlap:<10} {r.num_chunks:<10} {r. hit_rate:<12.2%} {r.mrr:<10.3f}")
    
    # Retrieval results
    ret_results = [r for r in results if r.component_type == "retrieval"]
    if ret_results:
        print(f"\n{'='*70}")
        print("ğŸ“Š Káº¾T QUáº¢:  RETRIEVAL STRATEGIES")
        print(f"{'='*70}")
        print(f"\n{'Config':<18} {'Type':<12} {'K':<6} {'Hit Rate':<12} {'MRR':<10} {'Latency':<10}")
        print("-" * 70)
        
        sorted_ret = sorted(ret_results, key=lambda x: x.mrr, reverse=True)
        for i, r in enumerate(sorted_ret):
            medal = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
            stype = r.config_details. get('search_type', '')
            k = r.config_details.get('k', 0)
            print(f"{medal} {r.config_name:<15} {stype:<12} {k:<6} {r.hit_rate:<12.2%} {r.mrr:<10.3f} {r.avg_latency:<10.3f}s")

def print_recommendations(results: List[BenchmarkResult]):
    """In khuyáº¿n nghá»‹"""
    print(f"\n{'='*70}")
    print("ğŸ¯ KHUYáº¾N NGHá»Š Cáº¤U HÃŒNH Tá»I Æ¯U")
    print(f"{'='*70}")
    
    # Best embedding
    emb = [r for r in results if r. component_type == "embedding" and r.mrr > 0]
    if emb:
        best = max(emb, key=lambda x: x.mrr)
        print(f"\n1ï¸âƒ£  EMBEDDING MODEL:")
        print(f"   âœ… {best.config_name}")
        print(f"      Model: {best.config_details. get('model_id', 'N/A')}")
        print(f"      MRR: {best.mrr:.3f} | Hit Rate: {best.hit_rate:.2%}")
    
    # Best chunking
    chunk = [r for r in results if r.component_type == "chunking" and r.mrr > 0]
    if chunk: 
        best = max(chunk, key=lambda x: x.mrr)
        print(f"\n2ï¸âƒ£  CHUNKING STRATEGY:")
        print(f"   âœ… {best.config_name}")
        print(f"      Chunk Size: {best.config_details['chunk_size']} | Overlap: {best. config_details['chunk_overlap']}")
        print(f"      MRR: {best.mrr:.3f} | Hit Rate: {best.hit_rate:.2%}")
    
    # Best retrieval
    ret = [r for r in results if r. component_type == "retrieval" and r.mrr > 0]
    if ret:
        best = max(ret, key=lambda x: x.mrr)
        print(f"\n3ï¸âƒ£  RETRIEVAL STRATEGY:")
        print(f"   âœ… {best.config_name}")
        print(f"      Type: {best.config_details['search_type']} | K: {best.config_details['k']}")
        print(f"      MRR: {best.mrr:.3f} | Hit Rate: {best.hit_rate:.2%}")
    
    print(f"\n{'='*70}")

def save_results(results: List[BenchmarkResult]):
    """LÆ°u káº¿t quáº£"""
    os.makedirs(RESULTS_PATH, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(RESULTS_PATH, f"components_benchmark_{timestamp}.json")
    
    data = {
        'timestamp': timestamp,
        'num_configs': len(results),
        'results': [asdict(r) for r in results]
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ÄÃ£ lÆ°u:  {filename}")

# ============== MAIN ==============

def main():
    print("\n" + "ğŸš€" + "=" * 68)
    print("   BENCHMARK TOÃ€N DIá»†N CÃC THÃ€NH PHáº¦N RAG")
    print("=" * 70)
    print(f"ğŸ“… Thá»i gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ Data path: {DATA_PATH}")
    print(f"ğŸ“‹ Test file: {TEST_DATASET_PATH}")
    print("=" * 70)
    
    # Load data
    print("\nğŸ“š Äang load dá»¯ liá»‡u...")
    documents = load_documents()
    test_cases = load_test_cases()
    print(f"   âœ… {len(documents)} documents, {len(test_cases)} test cases")
    
    all_results = []
    
    # 1. Benchmark Embeddings
    emb_results = benchmark_embeddings(documents, test_cases)
    all_results.extend(emb_results)
    
    # 2. Benchmark Chunking
    chunk_results = benchmark_chunking(documents, test_cases)
    all_results.extend(chunk_results)
    
    # 3. Benchmark Retrieval
    ret_results = benchmark_retrieval(documents, test_cases)
    all_results.extend(ret_results)
    
    # Print results
    print_results(all_results)
    print_recommendations(all_results)
    
    # Save
    save_results(all_results)
    
    print("\n" + "ğŸ‰" + "=" * 68)
    print("   BENCHMARK HOÃ€N Táº¤T!")
    print("=" * 70 + "\n")

if __name__ == "__main__":
    main()