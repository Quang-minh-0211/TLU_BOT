# chat. py - Phi√™n b·∫£n m·ªõi v·ªõi LCEL
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

CHROMA_PATH = "chroma_db"
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# Prompt template cho chatbot t∆∞ v·∫•n tuy·ªÉn sinh
PROMPT_TEMPLATE = """
B·∫°n l√† TLUBot - tr·ª£ l√Ω t∆∞ v·∫•n tuy·ªÉn sinh c·ªßa Tr∆∞·ªùng ƒê·∫°i h·ªçc Th·ªßy l·ª£i. 
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi.
N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y. 
Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, th√¢n thi·ªán v√† ch√≠nh x√°c.

Th√¥ng tin tham kh·∫£o:
{context}

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi:
"""

def load_vectorstore():
    """Load ChromaDB ƒë√£ t·∫°o"""
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'}
    )
    vectorstore = Chroma(
        persist_directory=CHROMA_PATH,
        embedding_function=embeddings
    )
    return vectorstore

def format_docs(docs):
    """Format documents th√†nh string"""
    return "\n\n".join(doc.page_content for doc in docs)

def create_rag_chain(vectorstore):
    """T·∫°o RAG chain v·ªõi LCEL"""
    # Kh·ªüi t·∫°o LLM
    llm = OllamaLLM(model="qwen2.5:7b")
    
    # T·∫°o retriever
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 5}
    )
    
    # T·∫°o prompt
    prompt = ChatPromptTemplate. from_template(PROMPT_TEMPLATE)
    
    # T·∫°o RAG chain v·ªõi LCEL
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain, retriever

def chat(rag_chain, question):
    """Tr·∫£ l·ªùi c√¢u h·ªèi"""
    response = rag_chain.invoke(question)
    return response

def main():
    print("ü§ñ TLU Chatbot - T∆∞ v·∫•n tuy·ªÉn sinh ƒê·∫°i h·ªçc Th·ªßy l·ª£i")
    print("=" * 50)
    print("üí° G√µ 'exit' ho·∫∑c 'q' ƒë·ªÉ tho√°t\n")
    
    # Load vector store v√† t·∫°o chain
    print("‚è≥ ƒêang kh·ªüi t·∫°o chatbot...")
    vectorstore = load_vectorstore()
    rag_chain, retriever = create_rag_chain(vectorstore)
    print("‚úÖ S·∫µn s√†ng!\n")
    
    while True:
        question = input("üë§ B·∫°n: ").strip()
        
        if question.lower() in ["exit", "quit", "q"]:
            print("üëã T·∫°m bi·ªát!")
            break
        
        if not question:
            continue
        
        print("\n‚è≥ ƒêang x·ª≠ l√Ω...")
        answer = chat(rag_chain, question)
        print(f"\nü§ñ TLUBot: {answer}\n")
        print("-" * 50)

if __name__ == "__main__":
    main()